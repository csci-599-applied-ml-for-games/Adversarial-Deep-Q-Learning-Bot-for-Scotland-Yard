{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "detectives_dqn_1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJZt6jWQly3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, defaultdict\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__SdK3V8mHpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ll1 = nn.Linear(1220, 716)\n",
        "        self.ll2 = nn.Linear(716, 716)\n",
        "        self.ll3 = nn.Linear(716, 358)\n",
        "        self.ll4 = nn.Linear(358, 358)\n",
        "        self.oll = nn.Linear(358, 275) #changed from 16 to 200\n",
        "    \n",
        "    #function implements a forward pass to the network\n",
        "    def forward(self, x):\n",
        "        # x = x.flatten(start_dim=1) - flattening needed only for images\n",
        "        x = F.relu(self.ll1(x))\n",
        "        x = F.relu(self.ll2(x))\n",
        "        x = F.relu(self.ll3(x))\n",
        "        x = F.relu(self.ll4(x))\n",
        "        x = F.softmax(self.oll(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx_PAmf4u7Wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Experience = namedtuple('Experience', ('state', 'action', 'next_state', 'reward'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eLgpFLimwxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory():\n",
        "    def __init__(self, capacity):\n",
        "      self.capacity = capacity\n",
        "      self.memory = []\n",
        "      self.push_count = 0 #keep track of how many experiences we've added to memory\n",
        "    \n",
        "    #function to store experiences as they occur in replay memory\n",
        "    def push(self, experience):\n",
        "      if len(self.memory) < self.capacity:\n",
        "          self.memory.append(experience)\n",
        "      else:\n",
        "          #if number of experiences exceeds capacity,\n",
        "          #push the new experience to the front of memory, overriding the oldest experiences\n",
        "          self.memory[self.push_count % self.capacity] = experience\n",
        "      self.push_count += 1\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "      return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def can_provide_sample(self, batch_size):\n",
        "      return len(self.memory) >= batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1BE4p1HnK8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EpsilonGreedyStrategy():\n",
        "    def __init__(self, start, end, decay):\n",
        "      self.start = start\n",
        "      self.end = end\n",
        "      self.decay = decay\n",
        "    \n",
        "    def get_exploration_rate(self, current_step):\n",
        "      return self.end + (self.start - self.end) * \\\n",
        "          math.exp(-1. * current_step * self.decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CUyzAPcnVTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self, strategy, num_actions, device):\n",
        "      self.current_step = 0\n",
        "      self.strategy = strategy\n",
        "      self.num_actions = num_actions\n",
        "      self.device = device\n",
        "      self.mappings = defaultdict(tuple)\n",
        "\n",
        "    def select_action(self, state, policy_net, possible_moves):\n",
        "      #policy network is the deep Q network that we train to get the optimal policy\n",
        "      rate = self.strategy.get_exploration_rate(self.current_step)\n",
        "      # print(\"Exploration rate: \", rate)\n",
        "      self.current_step += 1\n",
        "\n",
        "      if rate > random.random():\n",
        "        #print ('Exploring')\n",
        "        action = random.choice(possible_moves) #pick a possible out of the available ones - change\n",
        "        return torch.tensor(action[0]).to(self.device),rate # TODO - model action tensor    \n",
        "        \n",
        "      else:\n",
        "        #turning off gradient tracking during inference, not training\n",
        "        # print ('Exploiting')\n",
        "        with torch.no_grad():\n",
        "          #print(policy_net(state))\n",
        "          returned_states = policy_net(state)\n",
        "          max_value = float(\"-inf\")\n",
        "          max_index = 0\n",
        "          # print(\"POSSIBLE MOVES: \", possible_moves)\n",
        "          possible_moves_indexes = []\n",
        "          for i in possible_moves:\n",
        "            possible_moves_indexes.append(i[0])\n",
        "          for index, value in enumerate(returned_states):\n",
        "            if(index in possible_moves_indexes and value > max_value):\n",
        "              max_value = value\n",
        "              max_index = index\n",
        "          max_index_tensor = torch.tensor(max_index)\n",
        "          #result = policy_net(state).argmax(dim=-1).to(self.device)\n",
        "          # print ('RESULT', max_index_tensor)\n",
        "          return max_index_tensor, rate\n",
        "          #return policy_net(torch.tensor(state)).argmax(dim=1).to(self.device) # exploit\n",
        "\n",
        "    def select_action_testing(self, state, policy_net, possible_moves):    \n",
        "      with torch.no_grad():\n",
        "        #print(policy_net(state))\n",
        "        returned_states = policy_net(state)\n",
        "        max_value = float(\"-inf\")\n",
        "        max_index = 0\n",
        "        # print(\"POSSIBLE MOVES: \", possible_moves)\n",
        "        possible_moves_indexes = []\n",
        "        \n",
        "        for i in possible_moves:\n",
        "          possible_moves_indexes.append(i[0])\n",
        "        \n",
        "        for index, value in enumerate(returned_states):\n",
        "          if(index in possible_moves_indexes and value > max_value):\n",
        "            max_value = value\n",
        "            max_index = index\n",
        "        max_index_tensor = torch.tensor(max_index)\n",
        "        \n",
        "        #print ('Result',max_index_tensor)\n",
        "        return max_index_tensor\n",
        "        #return policy_net(torch.tensor(state)).argmax(dim=1).to(self.device) # exploit\n",
        "\n",
        "      '''\n",
        "      During training PyTorch keeps track of all the forward pass calculations that happen within the network. \n",
        "      It needs to do this so that it can know how to apply backpropagation later. \n",
        "      Since we’re only using the model for inference at the moment, we’re telling PyTorch not to keep track of any forward pass calculations.\n",
        "      - could this be a problem?\n",
        "      '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H32cg1Vrkcqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(value, size, max_value):\n",
        "      a = np.array(value)\n",
        "      b = np.zeros((size, max_value))\n",
        "      b[np.arange(size), a-1] = 1\n",
        "      return b.tolist()\n",
        "\n",
        "def generate_feature_space(initial_st):\n",
        "  #inital_st = [26, 5, [34, 29, 117, 174, 112], [[10, 8, 4], [10, 8, 4], [10, 8, 4], [10, 8, 4], [10, 8, 4]], 1]\n",
        "  #initial_st =  [0, 2, [41, 112, 198, 141, 174], [[9, 8, 4], [10, 8, 4], [10, 8, 4], [10, 8, 4], [10, 8, 4]], [1, 0, 0, 0, 0], 2]\n",
        "  feature_space = []\n",
        "  feature_space.extend(one_hot_encode(initial_st[0], 1, 199)[0]) #one hot encoded location of x\n",
        "  feature_space.append(initial_st[1]) #number of rounds until mr. x location is revealed\n",
        "  det_loc = one_hot_encode(initial_st[2], 5, 199) #location of detectives\n",
        "  feature_space.extend(det_loc[0])\n",
        "  feature_space.extend(det_loc[1])\n",
        "  feature_space.extend(det_loc[2])\n",
        "  feature_space.extend(det_loc[3])\n",
        "  feature_space.extend(det_loc[4])\n",
        "  det_tic = initial_st[3] #tokens with each detective\n",
        "  feature_space.extend(det_tic[0])\n",
        "  feature_space.extend(det_tic[1])\n",
        "  feature_space.extend(det_tic[2])\n",
        "  feature_space.extend(det_tic[3])\n",
        "  feature_space.extend(det_tic[4])\n",
        "  feature_space.extend(initial_st[4]) #resources used in the last five turns\n",
        "  feature_space.extend(one_hot_encode(initial_st[5], 1, 5)[0]) #dectective number \n",
        "\n",
        "  #print(feature_space)\n",
        "  #print(len(feature_space))\n",
        "  # print(one_hot_encode(initial_st[5], 1, 5)[0])\n",
        "  # print(len(feature_space))\n",
        "  return feature_space\n",
        "\n",
        "#generate_feature_space()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53sisbYmpet5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Environment():\n",
        "    def __init__(self, x_position, x_resources, detective_positions, detective_resources):\n",
        "      TAXI = 'TAXI'\n",
        "      BUS = 'BUS'\n",
        "      UNDERGROUND = 'UNDERGROUND'\n",
        "      BLACK = 'BLACK'\n",
        "      self.board = (  ( (8, (TAXI,)), (9, (TAXI,)), (58, (BUS,)), (46, (BUS, UNDERGROUND)) ),  # locToRoutes[0] has no value; need to access locToRoutes[1] and higher               \n",
        "         ( (8, (TAXI,)), (9, (TAXI,)), (58, (BUS,)), (46, (BUS, UNDERGROUND)) ),     # 001 \n",
        "         ( (20, (TAXI,)), (10, (TAXI,)) ),\n",
        "         ( (11, (TAXI,)), (12, (TAXI,)), (4, (TAXI,)), (22, (BUS,)), (23, (BUS,)) ),\n",
        "         ( (3, (TAXI,)), (13, (TAXI,)) ),\n",
        "         ( (15, (TAXI,)), (16, (TAXI,)) ),                                           # 005\n",
        "         ( (29, (TAXI,)), (7, (TAXI,)) ),\n",
        "         ( (6, (TAXI,)), (17, (TAXI,)), (42, (BUS,)) ),\n",
        "         ( (1, (TAXI,)), (19, (TAXI,)), (18, (TAXI,)) ),\n",
        "         ( (1, (TAXI,)), (19, (TAXI,)), (20, (TAXI,)) ),\n",
        "         ( (2, (TAXI,)), (11, (TAXI,)), (34, (TAXI,)), (21, (TAXI,)) ),              # 010\n",
        "         ( (3, (TAXI,)), (10, (TAXI,)), (22, (TAXI,)) ),\n",
        "         ( (3, (TAXI,)), (23, (TAXI,)) ),\n",
        "         ( (4, (TAXI,)), (14, (TAXI, BUS)), (24, (TAXI,)), (23, (TAXI, BUS)), (52, (BUS,)), \n",
        "            (89, (UNDERGROUND,)), (67, (UNDERGROUND,)), (46, (UNDERGROUND,)) ),\n",
        "         ( (13, (TAXI, BUS)), (15, (TAXI, BUS)), (25, (TAXI,)) ),\n",
        "         ( (5, (TAXI,)), (16, (TAXI,)), (28, (TAXI,)), (26, (TAXI,)), (14, (TAXI, BUS)),\n",
        "            (29, (BUS,)), (41, (BUS,)) ),                                            # 015\n",
        "         ( (5, (TAXI,)), (29, (TAXI,)), (28, (TAXI,)), (15, (TAXI,)) ),\n",
        "         ( (7, (TAXI,)), (30, (TAXI,)), (29, (TAXI,)) ),\n",
        "         ( (8, (TAXI,)), (31, (TAXI,)), (43, (TAXI,)) ),\n",
        "         ( (8, (TAXI,)), (9, (TAXI,)), (32, (TAXI,)) ),\n",
        "         ( (2, (TAXI,)), (9, (TAXI,)), (33, (TAXI,)) ),                              # 020\n",
        "         ( (10, (TAXI,)), (33, (TAXI,)) ),\n",
        "         ( (11, (TAXI,)), (23, (TAXI, BUS)), (35, (TAXI,)), (34, (TAXI, BUS)),\n",
        "            (3, (BUS,)), (65, (BUS,)) ),\n",
        "         ( (12, (TAXI,)), (13, (TAXI, BUS)), (37, (TAXI,)), (22, (TAXI, BUS)), \n",
        "            (3, (BUS,)), (67, (BUS,)) ),\n",
        "         ( (13, (TAXI,)), (38, (TAXI,)), (37, (TAXI,)) ),\n",
        "         ( (14, (TAXI,)), (39, (TAXI,)), (38, (TAXI,)) ),                            # 025\n",
        "         ( (15, (TAXI,)), (27, (TAXI,)), (39, (TAXI,)) ),\n",
        "         ( (26, (TAXI,)), (28, (TAXI,)), (40, (TAXI,)) ),\n",
        "         ( (15, (TAXI,)), (16, (TAXI,)), (41, (TAXI,)), (27, (TAXI,)) ),\n",
        "         ( (6, (TAXI,)), (17, (TAXI,)), (42, (TAXI, BUS)), (41, (TAXI, BUS)), (16, (TAXI,)),\n",
        "            (55, (BUS,)), (15, (BUS,)) ),\n",
        "         ( (17, (TAXI,)), (42, (TAXI,)) ),                                           # 030\n",
        "         ( (18, (TAXI,)), (44, (TAXI,)), (43, (TAXI,)) ),\n",
        "         ( (19, (TAXI,)), (33, (TAXI,)), (45, (TAXI,)), (44, (TAXI,)) ),\n",
        "         ( (20, (TAXI,)), (21, (TAXI,)), (46, (TAXI,)), (32, (TAXI,)) ),\n",
        "         ( (10, (TAXI,)), (22, (TAXI, BUS)), (48, (TAXI,)), (47, (TAXI,)), (63, (BUS,)),\n",
        "            (46, (BUS,)) ),\n",
        "         ( (22, (TAXI,)), (36, (TAXI,)), (65, (TAXI,)), (48, (TAXI,)) ),             # 035\n",
        "         ( (37, (TAXI,)), (49, (TAXI,)), (35, (TAXI,)) ),\n",
        "         ( (23, (TAXI,)), (24, (TAXI,)), (50, (TAXI,)), (36, (TAXI,)) ),\n",
        "         ( (24, (TAXI,)), (25, (TAXI,)), (51, (TAXI,)), (50, (TAXI,)) ),\n",
        "         ( (26, (TAXI,)), (52, (TAXI,)), (51, (TAXI,)), (25, (TAXI,)) ),\n",
        "         ( (27, (TAXI,)), (41, (TAXI,)), (53, (TAXI,)), (52, (TAXI,)) ),             # 040\n",
        "         ( (28, (TAXI,)), (29, (TAXI, BUS)), (54, (TAXI,)), (40, (TAXI,)),\n",
        "            (15, (BUS,)), (87, (BUS,)), (52, (BUS,)) ),\n",
        "         ( (30, (TAXI,)), (56, (TAXI,)), (72, (TAXI, BUS)), (29, (TAXI, BUS)),\n",
        "            (7, (BUS,)) ),\n",
        "         ( (18, (TAXI,)), (31, (TAXI,)), (57, (TAXI,)) ),\n",
        "         ( (32, (TAXI,)), (58, (TAXI,)), (31, (TAXI,)) ),\n",
        "         ( (32, (TAXI,)), (46, (TAXI,)), (60, (TAXI,)), (59, (TAXI,)),               # 045\n",
        "            (58, (TAXI,)) ),\n",
        "         ( (33, (TAXI,)), (47, (TAXI,)), (61, (TAXI,)), (45, (TAXI,)), (34, (BUS,)),\n",
        "            (78, (BUS,)), (58, (BUS,)), (1, (BUS, UNDERGROUND)), (13, (UNDERGROUND,)),\n",
        "            (79, (UNDERGROUND,)), (74, (UNDERGROUND,)) ),\n",
        "         ( (34, (TAXI,)), (62, (TAXI,)), (46, (TAXI,)) ),\n",
        "         ( (34, (TAXI,)), (35, (TAXI,)), (63, (TAXI,)), (62, (TAXI,)) ),\n",
        "         ( (36, (TAXI,)), (50, (TAXI,)), (66, (TAXI,)) ),\n",
        "         ( (37, (TAXI,)), (38, (TAXI,)), (49, (TAXI,)) ),                            # 050\n",
        "         ( (38, (TAXI,)), (39, (TAXI,)), (52, (TAXI,)), (68, (TAXI,)), (67, (TAXI,)) ),\n",
        "         ( (39, (TAXI,)), (40, (TAXI,)), (69, (TAXI,)), (51, (TAXI,)), (13, (BUS,)),\n",
        "            (41, (BUS,)), (86, (BUS,)), (67, (BUS,)) ),\n",
        "         ( (40, (TAXI,)), (54, (TAXI,)), (69, (TAXI,)) ),\n",
        "         ( (41, (TAXI,)), (55, (TAXI,)), (70, (TAXI,)), (53, (TAXI,)) ),\n",
        "         ( (71, (TAXI,)), (54, (TAXI,)), (29, (BUS,)), (89, (BUS,)) ),               # 055\n",
        "         ( (42, (TAXI,)), (91, (TAXI,)) ),\n",
        "         ( (43, (TAXI,)), (58, (TAXI,)), (73, (TAXI,)) ),\n",
        "         ( (45, (TAXI,)), (59, (TAXI,)), (75, (TAXI,)), (74, (TAXI, BUS)), (57, (TAXI,)),\n",
        "            (44, (TAXI,)), (46, (BUS,)), (77, (BUS,)), (1, (BUS,)) ),\n",
        "         ( (45, (TAXI,)), (76, (TAXI,)), (75, (TAXI,)), (58, (TAXI,)) ),\n",
        "         ( (45, (TAXI,)), (61, (TAXI,)), (76, (TAXI,)) ),                            # 060\n",
        "         ( (46, (TAXI,)), (62, (TAXI,)), (78, (TAXI,)), (76, (TAXI,)), (60, (TAXI,)) ),\n",
        "         ( (47, (TAXI,)), (48, (TAXI,)), (79, (TAXI,)), (61, (TAXI,)) ),\n",
        "         ( (48, (TAXI,)), (64, (TAXI,)), (80, (TAXI,)), (79, (TAXI, BUS)),\n",
        "            (34, (BUS,)), (65, (BUS,)), (100, (BUS,)) ),\n",
        "         ( (65, (TAXI,)), (81, (TAXI,)), (63, (TAXI,)) ),\n",
        "         ( (35, (TAXI,)), (66, (TAXI,)), (82, (TAXI, BUS)), (64, (TAXI,)),           # 065\n",
        "            (22, (BUS,)), (67, (BUS,)), (63, (BUS,)) ),                                           \n",
        "         ( (49, (TAXI,)), (67, (TAXI,)), (82, (TAXI,)), (65, (TAXI,)) ),\n",
        "         ( (51, (TAXI,)), (68, (TAXI,)), (84, (TAXI,)), (66, (TAXI,)), (23, (BUS,)),\n",
        "            (52, (BUS,)), (102, (BUS,)), (82, (BUS,)), (65, (BUS,)),\n",
        "            (13, (UNDERGROUND,)), (89, (UNDERGROUND,)), (111, (UNDERGROUND,)),\n",
        "            (79, (UNDERGROUND,)) ),\n",
        "         ( (51, (TAXI,)), (69, (TAXI,)), (85, (TAXI,)), (67, (TAXI,)) ),\n",
        "         ( (52, (TAXI,)), (53, (TAXI,)), (86, (TAXI,)), (68, (TAXI,)) ),\n",
        "         ( (54, (TAXI,)), (71, (TAXI,)), (87, (TAXI,)) ),                            # 070\n",
        "         ( (55, (TAXI,)), (72, (TAXI,)), (89, (TAXI,)), (70, (TAXI,)) ),\n",
        "         ( (42, (TAXI, BUS)), (91, (TAXI,)), (90, (TAXI,)), (71, (TAXI,)),\n",
        "            (107, (BUS,)), (105, (BUS,)) ),\n",
        "         ( (57, (TAXI,)), (74, (TAXI,)), (92, (TAXI,)) ),\n",
        "         ( (58, (TAXI, BUS)), (75, (TAXI,)), (92, (TAXI,)), (73, (TAXI,)),\n",
        "            (94, (BUS,)), (46, (UNDERGROUND,)) ),\n",
        "         ( (58, (TAXI,)), (59, (TAXI,)), (94, (TAXI,)), (74, (TAXI,)) ),             # 075\n",
        "         ( (59, (TAXI,)), (60, (TAXI,)), (61, (TAXI,)), (77, (TAXI,)) ),\n",
        "         ( (78, (TAXI, BUS)), (96, (TAXI,)), (95, (TAXI,)), (76, (TAXI,)),\n",
        "            (124, (BUS,)), (94, (BUS,)), (58, (BUS,)) ),\n",
        "         ( (61, (TAXI,)), (79, (TAXI, BUS)), (97, (TAXI,)), (77, (TAXI, BUS)),\n",
        "            (46, (BUS,)) ),\n",
        "         ( (62, (TAXI,)), (63, (TAXI, BUS)), (98, (TAXI,)), (78, (TAXI, BUS)),\n",
        "            (46, (UNDERGROUND,)), (67, (UNDERGROUND,)), (111, (UNDERGROUND,)),\n",
        "            (93, (UNDERGROUND,)) ),\n",
        "         ( (63, (TAXI,)), (100, (TAXI,)), (99, (TAXI,)) ),                           # 080\n",
        "         ( (64, (TAXI,)), (82, (TAXI,)), (100, (TAXI,)) ),\n",
        "         ( (65, (TAXI, BUS)), (66, (TAXI,)), (67, (BUS,)), (101, (TAXI,)), (140, (BUS,)),\n",
        "            (81, (TAXI,)), (100, (BUS,)) ),\n",
        "         ( (102, (TAXI,)), (101, (TAXI,)) ),\n",
        "         ( (67, (TAXI,)), (85, (TAXI,)) ),\n",
        "         ( (68, (TAXI,)), (103, (TAXI,)), (84, (TAXI,)) ),                           # 085\n",
        "         ( (69, (TAXI,)), (52, (BUS,)), (87, (BUS,)), (104, (TAXI,)), (116, (BUS,)),\n",
        "            (103, (TAXI,)), (102, (BUS,)) ),\n",
        "         ( (70, (TAXI,)), (41, (BUS,)), (88, (TAXI,)), (105, (BUS,)), (86, (BUS,)) ),\n",
        "         ( (89, (TAXI,)), (117, (TAXI,)), (87, (TAXI,)) ),\n",
        "         ( (71, (TAXI,)), (55, (BUS,)), (13, (UNDERGROUND,)), (105, (TAXI, BUS)),\n",
        "            (128, (UNDERGROUND,)), (88, (TAXI,)), (140, (UNDERGROUND,)),\n",
        "            (67, (UNDERGROUND,)) ),\n",
        "         ( (72, (TAXI,)), (91, (TAXI,)), (105, (TAXI,)) ),                           # 090 \n",
        "         ( (56, (TAXI,)), (107, (TAXI,)), (105, (TAXI,)), (90, (TAXI,)), (72, (TAXI,)) ),\n",
        "         ( (73, (TAXI,)), (74, (TAXI,)), (93, (TAXI,)) ),\n",
        "         ( (92, (TAXI,)), (94, (TAXI, BUS)), (79, (UNDERGROUND,)) ),\n",
        "         ( (74, (BUS,)), (75, (TAXI,)), (95, (TAXI,)), (77, (BUS,)), (93, (TAXI, BUS)) ),\n",
        "         ( (77, (TAXI,)), (122, (TAXI,)), (94, (TAXI,)) ),                           # 095\n",
        "         ( (77, (TAXI,)), (97, (TAXI,)), (109, (TAXI,)) ),\n",
        "         ( (78, (TAXI,)), (98, (TAXI,)), (109, (TAXI,)), (96, (TAXI,)) ),\n",
        "         ( (79, (TAXI,)), (99, (TAXI,)), (110, (TAXI,)), (97, (TAXI,)) ),\n",
        "         ( (80, (TAXI,)), (112, (TAXI,)), (110, (TAXI,)), (98, (TAXI,)) ),\n",
        "         ( (81, (TAXI,)), (82, (BUS,)), (101, (TAXI,)), (113, (TAXI,)),              # 100\n",
        "            (112, (TAXI,)), (111, (BUS,)), (80, (TAXI,)), (63, (BUS,)) ),\n",
        "         ( (83, (TAXI,)), (114, (TAXI,)), (100, (TAXI,)), (82, (TAXI,)) ),\n",
        "         ( (67, (BUS,)), (103, (TAXI,)), (86, (BUS,)), (115, (TAXI,)), (127, (BUS,)),\n",
        "            (83, (TAXI,)) ),\n",
        "         ( (85, (TAXI,)), (86, (TAXI,)), (102, (TAXI,)) ),\n",
        "         ( (86, (TAXI,)), (116, (TAXI,)) ),\n",
        "         ( (90, (TAXI,)), (72, (BUS,)), (91, (TAXI,)), (106, (TAXI,)),               # 105\n",
        "            (107, (BUS,)), (108, (TAXI, BUS)), (87, (BUS,)), (89, (TAXI, BUS)) ),\n",
        "         ( (107, (TAXI,)), (105, (TAXI,)) ),\n",
        "         ( (91, (TAXI,)), (72, (BUS,)), (119, (TAXI,)), (161, (BUS,)),\n",
        "            (106, (TAXI,)), (105, (BUS,)) ),\n",
        "         ( (105, (TAXI, BUS)), (119, (TAXI,)), (135, (BUS,)), (117, (TAXI,)),\n",
        "            (116, (BUS,)), (115, (BLACK,)) ),\n",
        "         ( (97, (TAXI,)), (110, (TAXI,)), (124, (TAXI,)), (96, (TAXI,)) ),\n",
        "         ( (99, (TAXI,)), (111, (TAXI,)), (109, (TAXI,)), (98, (TAXI,)) ),           # 110\n",
        "         ( (112, (TAXI,)), (100, (BUS,)), (67, (UNDERGROUND,)),\n",
        "            (153, (UNDERGROUND,)), (124, (TAXI, BUS)), (163, (UNDERGROUND,)),\n",
        "            (110, (TAXI,)), (79, (UNDERGROUND,)) ),\n",
        "         ( (100, (TAXI,)), (125, (TAXI,)), (111, (TAXI,)), (99, (TAXI,)) ),\n",
        "         ( (114, (TAXI,)), (125, (TAXI,)), (100, (TAXI,)) ),\n",
        "         ( (101, (TAXI,)), (115, (TAXI,)), (126, (TAXI,)),\n",
        "            (132, (TAXI,)), (131, (TAXI,)), (113, (TAXI,)) ),\n",
        "         ( (102, (TAXI,)), (127, (TAXI,)), (126, (TAXI,)), (114, (TAXI,)),           # 115\n",
        "            (108, (BLACK,)), (157, (BLACK,)) ),\n",
        "         ( (104, (TAXI,)), (86, (BUS,)), (117, (TAXI,)), (108, (BUS,)),\n",
        "            (118, (TAXI,)), (142, (BUS,)), (127, (TAXI, BUS)) ),\n",
        "         ( (88, (TAXI,)), (108, (TAXI,)), (129, (TAXI,)), (116, (TAXI,)) ),\n",
        "         ( (116, (TAXI,)), (129, (TAXI,)), (142, (TAXI,)), (134, (TAXI,)) ),\n",
        "         ( (107, (TAXI,)), (136, (TAXI,)), (108, (TAXI,)) ),\n",
        "         ( (121, (TAXI,)), (144, (TAXI,)) ),                                         # 120\n",
        "         ( (122, (TAXI,)), (145, (TAXI,)), (120, (TAXI,)) ),\n",
        "         ( (95, (TAXI,)), (123, (TAXI, BUS)), (146, (TAXI,)),\n",
        "            (121, (TAXI,)), (144, (BUS,)) ),\n",
        "         ( (124, (TAXI, BUS)), (149, (TAXI,)), (165, (BUS,)), (148, (TAXI,)),\n",
        "            (137, (TAXI,)), (144, (BUS,)), (122, (TAXI, BUS)) ),\n",
        "         ( (109, (TAXI,)), (111, (TAXI, BUS)), (130, (TAXI,)), (138, (TAXI,)),\n",
        "            (153, (BUS,)), (123, (TAXI, BUS)), (77, (BUS,)) ),\n",
        "         ( (113, (TAXI,)), (131, (TAXI,)), (112, (TAXI,)) ),                         # 125\n",
        "         ( (115, (TAXI,)), (127, (TAXI,)), (140, (TAXI,)), (114, (TAXI,)) ),\n",
        "         ( (116, (TAXI, BUS)), (134, (TAXI,)), (133, (TAXI, BUS)),\n",
        "            (126, (TAXI,)), (115, (TAXI,)), (102, (BUS,)) ),\n",
        "         ( (143, (TAXI,)), (135, (BUS,)), (89, (UNDERGROUND,)), (160, (TAXI,)),\n",
        "            (161, (BUS,)), (188, (TAXI,)), (199, (BUS,)), (172, (TAXI,)),\n",
        "            (187, (BUS,)), (185, (UNDERGROUND,)), (142, (TAXI, BUS)),\n",
        "            (140, (UNDERGROUND,)) ),\n",
        "         ( (117, (TAXI,)), (135, (TAXI,)), (143, (TAXI,)), (142, (TAXI,)),\n",
        "            (118, (TAXI,)) ),\n",
        "         ( (131, (TAXI,)), (139, (TAXI,)), (124, (TAXI,)) ),                         # 130\n",
        "         ( (114, (TAXI,)), (130, (TAXI,)), (125, (TAXI,)) ),\n",
        "         ( (114, (TAXI,)), (140, (TAXI,)) ),\n",
        "         ( (127, (TAXI, BUS)), (141, (TAXI,)), (157, (BUS,)), (140, (TAXI, BUS)) ),\n",
        "         ( (118, (TAXI,)), (142, (TAXI,)), (141, (TAXI,)), (127, (TAXI,)) ),\n",
        "         ( (108, (BUS,)), (136, (TAXI,)), (161, (TAXI, BUS)), (143, (TAXI,)),        # 135\n",
        "            (128, (BUS,)), (129, (TAXI,)) ),\n",
        "         ( (119, (TAXI,)), (162, (TAXI,)), (135, (TAXI,)) ),\n",
        "         ( (123, (TAXI,)), (147, (TAXI,)) ),\n",
        "         ( (152, (TAXI,)), (150, (TAXI,)), (124, (TAXI,)) ),\n",
        "         ( (130, (TAXI,)), (140, (TAXI,)), (154, (TAXI,)), (153, (TAXI,)) ),\n",
        "         ( (132, (TAXI,)), (82, (BUS,)), (126, (TAXI,)), (89, (UNDERGROUND,)),       # 140\n",
        "            (133, (TAXI, BUS)), (128, (UNDERGROUND,)), (156, (TAXI, BUS)),\n",
        "            (154, (TAXI, BUS)), (153, (UNDERGROUND,)), (139, (TAXI,)) ),\n",
        "         ( (134, (TAXI,)), (142, (TAXI,)), (158, (TAXI,)), (133, (TAXI,)) ),\n",
        "         ( (118, (TAXI,)), (116, (BUS,)), (129, (TAXI,)), (143, (TAXI,)),\n",
        "            (128, (TAXI, BUS)), (158, (TAXI,)), (157, (BUS,)), (141, (TAXI,)),\n",
        "            (134, (TAXI,)) ),\n",
        "         ( (135, (TAXI,)), (160, (TAXI,)), (128, (TAXI,)), (142, (TAXI,)),\n",
        "            (129, (TAXI,)) ),\n",
        "         ( (120, (TAXI,)), (122, (BUS,)), (145, (TAXI,)), (123, (BUS,)),\n",
        "            (163, (BUS,)), (177, (TAXI,)) ),\n",
        "         ( (121, (TAXI,)), (146, (TAXI,)), (144, (TAXI,)) ),                         # 145\n",
        "         ( (122, (TAXI,)), (147, (TAXI,)), (163, (TAXI,)), (145, (TAXI,)) ),\n",
        "         ( (137, (TAXI,)), (164, (TAXI,)), (146, (TAXI,)) ),\n",
        "         ( (123, (TAXI,)), (149, (TAXI,)), (164, (TAXI,)) ),\n",
        "         ( (123, (TAXI,)), (150, (TAXI,)), (165, (TAXI,)), (148, (TAXI,)) ),\n",
        "         ( (138, (TAXI,)), (151, (TAXI,)), (149, (TAXI,)) ),                         # 150\n",
        "         ( (152, (TAXI,)), (166, (TAXI,)), (165, (TAXI,)), (150, (TAXI,)) ),\n",
        "         ( (153, (TAXI,)), (151, (TAXI,)), (138, (TAXI,)) ),\n",
        "         ( (139, (TAXI,)), (111, (UNDERGROUND,)), (154, (TAXI, BUS)),\n",
        "            (140, (UNDERGROUND,)), (167, (TAXI,)), (184, (BUS,)),\n",
        "            (185, (UNDERGROUND,)), (166, (TAXI,)), (180, (BUS,)),\n",
        "            (163, (UNDERGROUND,)), (152, (TAXI,)), (124, (BUS,)) ),\n",
        "         ( (140, (TAXI, BUS)), (155, (TAXI,)), (156, (BUS,)), (153, (TAXI, BUS)),\n",
        "            (139, (TAXI,)) ),\n",
        "         ( (156, (TAXI,)), (168, (TAXI,)), (167, (TAXI,)), (154, (TAXI,)) ),         # 155\n",
        "         ( (140, (TAXI, BUS)), (157, (TAXI, BUS)), (169, (TAXI,)),\n",
        "            (184, (BUS,)), (155, (TAXI,)), (154, (BUS,)) ),\n",
        "         ( (133, (BUS,)), (158, (TAXI,)), (142, (BUS,)), (170, (TAXI,)),\n",
        "            (185, (BUS,)), (156, (TAXI, BUS)), (115, (BLACK,)), (194, (BLACK,)) ),\n",
        "         ( (141, (TAXI,)), (142, (TAXI,)), (159, (TAXI,)), (157, (TAXI,)) ),\n",
        "         ( (158, (TAXI,)), (172, (TAXI,)), (198, (TAXI,)), (186, (TAXI,)),\n",
        "            (170, (TAXI,)) ),\n",
        "         ( (143, (TAXI,)), (161, (TAXI,)), (173, (TAXI,)), (128, (TAXI,)) ),         # 160\n",
        "         ( (107, (BUS,)), (174, (TAXI,)), (199, (BUS,)), (160, (TAXI,)),\n",
        "            (128, (BUS,)), (135, (TAXI, BUS)) ),\n",
        "         ( (175, (TAXI,)), (136, (TAXI,)) ),\n",
        "         ( (146, (TAXI,)), (111, (UNDERGROUND,)), (153, (UNDERGROUND,)),\n",
        "            (191, (BUS,)), (177, (TAXI,)), (176, (BUS,)), (144, (BUS,)) ),\n",
        "         ( (147, (TAXI,)), (148, (TAXI,)), (179, (TAXI,)), (178, (TAXI,)) ),\n",
        "         ( (149, (TAXI,)), (123, (BUS,)), (151, (TAXI,)), (180, (TAXI, BUS)),        # 165\n",
        "            (179, (TAXI,)), (191, (BUS,)) ),\n",
        "         ( (153, (TAXI,)), (183, (TAXI,)), (181, (TAXI,)), (151, (TAXI,)) ),\n",
        "         ( (155, (TAXI,)), (168, (TAXI,)), (183, (TAXI,)), (153, (TAXI,)) ),\n",
        "         ( (155, (TAXI,)), (184, (TAXI,)), (167, (TAXI,)) ),\n",
        "         ( (156, (TAXI,)), (184, (TAXI,)) ),\n",
        "         ( (157, (TAXI,)), (159, (TAXI,)), (185, (TAXI,)) ),                         # 170\n",
        "         ( (173, (TAXI,)), (175, (TAXI,)), (199, (TAXI,)) ),\n",
        "         ( (128, (TAXI,)), (187, (TAXI,)), (159, (TAXI,)) ),\n",
        "         ( (160, (TAXI,)), (174, (TAXI,)), (171, (TAXI,)), (188, (TAXI,)) ),\n",
        "         ( (175, (TAXI,)), (173, (TAXI,)), (161, (TAXI,)) ),\n",
        "         ( (162, (TAXI,)), (171, (TAXI,)), (174, (TAXI,)) ),                         # 175\n",
        "         ( (177, (TAXI,)), (163, (BUS,)), (189, (TAXI,)), (190, (BUS,)) ),\n",
        "         ( (144, (TAXI,)), (163, (TAXI,)), (176, (TAXI,)) ),\n",
        "         ( (164, (TAXI,)), (191, (TAXI,)), (189, (TAXI,)) ),\n",
        "         ( (165, (TAXI,)), (191, (TAXI,)), (164, (TAXI,)) ),\n",
        "         ( (165, (TAXI, BUS)), (181, (TAXI,)), (153, (BUS,)), (193, (TAXI,)),        # 180\n",
        "            (184, (BUS,)), (190, (BUS,)) ),\n",
        "         ( (166, (TAXI,)), (182, (TAXI,)), (193, (TAXI,)), (180, (TAXI,)) ),\n",
        "         ( (183, (TAXI,)), (195, (TAXI,)), (181, (TAXI,)) ), \n",
        "         ( (167, (TAXI,)), (196, (TAXI,)), (182, (TAXI,)), (166, (TAXI,)) ),\n",
        "         ( (169, (TAXI,)), (156, (BUS,)), (185, (TAXI, BUS)), (197, (TAXI,)),\n",
        "            (196, (TAXI,)), (180, (BUS,)), (168, (TAXI,)), (153, (BUS,)) ),\n",
        "         ( (170, (TAXI,)), (157, (BUS,)), (186, (TAXI,)), (187, (BUS,)),             # 185\n",
        "            (128, (UNDERGROUND,)), (184, (TAXI, BUS)), (153, (UNDERGROUND,)) ),\n",
        "         ( (159, (TAXI,)), (198, (TAXI,)), (185, (TAXI,)) ),\n",
        "         ( (172, (TAXI,)), (128, (BUS,)), (188, (TAXI,)), (198, (TAXI,)),\n",
        "            (185, (BUS,)) ),\n",
        "         ( (128, (TAXI,)), (173, (TAXI,)), (199, (TAXI,)), (187, (TAXI,)) ),\n",
        "         ( (178, (TAXI,)), (190, (TAXI,)), (176, (TAXI,)) ),\n",
        "         ( (191, (TAXI, BUS)), (192, (TAXI,)), (180, (BUS,)),                        # 190\n",
        "            (189, (TAXI,)), (176, (BUS,)) ),\n",
        "         ( (179, (TAXI,)), (165, (BUS,)), (192, (TAXI,)), (190, (TAXI, BUS)),\n",
        "            (178, (TAXI,)), (163, (BUS,)) ),\n",
        "         ( (191, (TAXI,)), (194, (TAXI,)), (190, (TAXI,)) ),\n",
        "         ( (181, (TAXI,)), (194, (TAXI,)), (180, (TAXI,)) ),\n",
        "         ( (195, (TAXI,)), (192, (TAXI,)), (193, (TAXI,)), (157, (BLACK,)) ),\n",
        "         ( (182, (TAXI,)), (197, (TAXI,)), (194, (TAXI,)) ),                         # 195\n",
        "         ( (183, (TAXI,)), (184, (TAXI,)), (197, (TAXI,)) ),\n",
        "         ( (196, (TAXI,)), (184, (TAXI,)), (195, (TAXI,)) ),\n",
        "         ( (159, (TAXI,)), (187, (TAXI,)), (199, (TAXI,)), (186, (TAXI,)) ),\n",
        "         ( (188, (TAXI,)), (128, (BUS,)), (171, (TAXI,)), (161, (BUS,)),\n",
        "            (198, (TAXI,)) ) \n",
        "      )\n",
        "      \n",
        "      self.x_position = x_position\n",
        "      self.detective_positions = detective_positions\n",
        "      self.x_resources =  x_resources\n",
        "      #self.detective_resources = detective_resources\n",
        "      self.detective_resources = [[10, 8, 4], [10, 8, 4], [10, 8, 4], [10, 8, 4], [10, 8, 4]]\n",
        "      self.done = False\n",
        "      self.possible_moves = self.board[self.x_position]\n",
        "   \n",
        "    def take_action_detectives_random(self):\n",
        "      d = {\n",
        "          \"TAXI\":0,\n",
        "           \"BUS\":1,\n",
        "           \"UNDERGROUND\":2,\n",
        "           \"BLACK\":3\n",
        "      }\n",
        "      # print(self.detective_resources)\n",
        "      for index, each_detective in enumerate(self.detective_positions):\n",
        "        pm = self.board[each_detective]\n",
        "        new_pm = []\n",
        "        for x in pm:\n",
        "          for k in range(0, len(x[1])):\n",
        "            if(x[1][k] != \"BLACK\"):\n",
        "\n",
        "              if(self.detective_resources[index][d[x[1][k]]] > 0):\n",
        "                new_pm.append((x[0], x[1][k]))\n",
        "        if(len(new_pm) > 0):\n",
        "          random_move = random.choice(new_pm)\n",
        "        \n",
        "          # print(random_move)\n",
        "          if 'TAXI' == random_move[1]:\n",
        "            resource_used = 0\n",
        "            \n",
        "          elif 'BUS' == random_move[1]:\n",
        "            resource_used = 1\n",
        "          \n",
        "          else:\n",
        "            resource_used = 2\n",
        "\n",
        "          \n",
        "          self.detective_positions[index] = random_move[0]\n",
        "          self.detective_resources[index][resource_used]-=1\n",
        "      \n",
        "\n",
        "    def take_action_detectives(self):\n",
        "      res_state_loc = []\n",
        "      res_state_resources = self.detective_resources\n",
        "\n",
        "      for count, det in enumerate(self.detective_positions):\n",
        "        moves = self.board[int(det)]\n",
        "        min_dist_node = -1\n",
        "        min_dist_val = float(\"inf\")\n",
        "        \n",
        "        for move in moves:\n",
        "          val = self.getdistance(self.x_position, move[0])\n",
        "          \n",
        "          if(val<min_dist_val):\n",
        "            #TODO: MAKE SURE DETECTIVE DONT MOVE IF THERE ARE NO RESOURCES\n",
        "            min_dist_node = move\n",
        "            min_dist_val = val\n",
        "\n",
        "        res_state_loc.append(min_dist_node[0])\n",
        "        # print(\"COUNT: \", count)\n",
        "        if(min_dist_node[1][0] == 'TAXI'):\n",
        "          res_state_resources[count][0] = max(res_state_resources[count][0] -1,0)\n",
        "\n",
        "        elif(min_dist_node[1][0] == 'BUS'):\n",
        "          res_state_resources[count][1] = max(res_state_resources[count][1] -1,0)        \n",
        "        else:\n",
        "          res_state_resources[count][2] = max(res_state_resources[count][2] -1,0)      \n",
        "      return (res_state_loc, res_state_resources)\n",
        "      \n",
        "    def get_state(self, round_number):\n",
        "      #TODO tokens_used_in_5_turns, detective_number\n",
        "      in_state = [self.x_position, round_number%5, self.detective_positions, self.detective_resources, [1,1,1,1,1], 1] \n",
        "      state = generate_feature_space(in_state)\n",
        "      return torch.tensor(state)\n",
        "    \n",
        "    def getdistance(self, x, y):\n",
        "      #TODO\n",
        "      return abs(int(x) - int(y))\n",
        "\n",
        "    def num_actions_available(self):\n",
        "      return len(self.possible_moves)\n",
        "\n",
        "    def take_action(self, action, timestep):\n",
        "      #update Mr. X's position to simulate his taking an action\n",
        "      self.x_position = action\n",
        "\n",
        "      '''det_actions = take_action_detectives()\n",
        "      self.detective_positions = det_actions[0]\n",
        "      self.detective_resources = det_actions[1]'''\n",
        "\n",
        "      self.take_action_detectives_random()\n",
        "      \n",
        "      # detective_position_average = 0\n",
        "      # for each_detective in self.detective_positions:\n",
        "      #   detective_position_average += each_detective\n",
        "      # detective_position_average = detective_position_average/5\n",
        "\n",
        "      # reward = self.getdistance(self.x_position, detective_position_average)\n",
        "      self.done = self.is_done(timestep)\n",
        "      if self.done == 1:\n",
        "        return -100\n",
        "      elif(self.done == 2):\n",
        "        return 100\n",
        "      else:\n",
        "        return 0\n",
        "    \n",
        "     \n",
        "    def is_done(self, timestep):\n",
        "      #game ends when either one detective is at the place of Mr. X\n",
        "      for each_detective in self.detective_positions:\n",
        "        if each_detective == self.x_position:\n",
        "          return 1\n",
        "      if(timestep == 24):\n",
        "        return 2\n",
        "      return 0\n",
        "    #returns true is mr_x wins \n",
        "    def winner(self):\n",
        "\n",
        "      for each_detective in self.detective_positions:\n",
        "        if each_detective == self.x_position:\n",
        "          return False\n",
        "      return True\n",
        " \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASWIatSsxpMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QValues():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    @staticmethod\n",
        "    def get_current(policy_net, states, actions):\n",
        "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
        "    @staticmethod        \n",
        "    def get_next(target_net, next_states):                \n",
        "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
        "        non_final_state_locations = (final_state_locations == False)\n",
        "        non_final_states = next_states[non_final_state_locations]\n",
        "        batch_size = next_states.shape[0]\n",
        "        values = torch.zeros(batch_size).to(QValues.device)\n",
        "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
        "        return values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQbUyXQBZ5VT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_tensors(experiences):\n",
        "    # Convert batch of Experiences to Experience of batches\n",
        "    batch = Experience(*zip(*experiences))\n",
        "    #print ('BATCH',batch)\n",
        "    t1 = torch.stack(batch.state)\n",
        "    t4 = torch.stack(batch.next_state)\n",
        "    t3 = torch.stack(batch.reward)\n",
        "    t2 = torch.stack(batch.action)\n",
        "    #t3 = torch.cat(batch.reward)\n",
        "    #t4 = torch.cat(batch.next_state)\n",
        "\n",
        "    return (t1,t2,t3,t4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aD54EhugXHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_training(rates,performance):\n",
        "  # plotting the points  \n",
        "  plt.plot(rates, performance, color='green', linestyle='solid', linewidth = 3)\n",
        "  \n",
        "  # naming the x axis \n",
        "  plt.xlabel('Explore') \n",
        "  # naming the y axis \n",
        "  plt.ylabel('Detective Wins - Mr. X Wins') \n",
        "\n",
        "  # giving a title to the graph \n",
        "  plt.title('Training') \n",
        "\n",
        "  # invert the x-axis on the graph\n",
        "  #plt.gca().invert_xaxis()\n",
        "  \n",
        "  # function to show the plot \n",
        "  plt.show() \n",
        "\n",
        "def plot_testing(performance,games):\n",
        "  # plotting the points  \n",
        "  plt.plot(games, performance, color='red', linestyle='solid', linewidth = 3)\n",
        "  \n",
        "  # naming the x axis \n",
        "  plt.xlabel('Game Number') \n",
        "\n",
        "  # naming the y axis \n",
        "  plt.ylabel('Detective Wins - Mr. X Wins') \n",
        "\n",
        "  # giving a title to my graph \n",
        "  plt.title('Testing') \n",
        "\n",
        "  # function to show the plot \n",
        "  plt.show() \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDsyDPg6bkrL",
        "colab_type": "code",
        "outputId": "d942ff40-2873-4e81-aacf-f3edbc522d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "source": [
        "'''\n",
        "Should X position be tensor or not, and how to pass action tuple + need to handle multiple modes of transport in action.\n",
        "'''\n",
        "\n",
        "batch_size = 10 #256\n",
        "gamma = 0.999\n",
        "eps_start = 1\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.001\n",
        "target_update = 10 # how often we update target with policy n/w\n",
        "memory_size = 100000 #check with paper\n",
        "lr = 0.001\n",
        "num_episodes = 50 #1000\n",
        "# state_dim = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "start_state = [100, 5, [41, 112, 198, 141, 174], [[10, 8, 4], [10, 8, 4], [10, 8, 4], [10, 8, 4], [10, 8, 4]], [1, 0, 0, 0, 0], 1]\n",
        "\n",
        "\n",
        "start_feature = generate_feature_space(start_state)\n",
        "#print ('Length',len(start_feature))\n",
        "#print ('Without tensor',start_feature)\n",
        "#print ('With tensor',torch.tensor(start_feature).size())\n",
        "\n",
        "em = Environment(start_state[0], start_state[1], start_state[2], start_state[3])\n",
        "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
        "\n",
        "agent = Agent(strategy, em.num_actions_available(), device)\n",
        "#print ('No. of available actions: ', em.num_actions_available())\n",
        "memory = ReplayMemory(memory_size)\n",
        "\n",
        "policy_net = DQN().to(device)\n",
        "target_net = DQN().to(device)\n",
        "\n",
        "#Set weights and biases in target_net to be same as that in policy_net\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "#eval mode, which tells PyTorch that this network is not in training mode. In other words, this network will only be used for inference\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
        "\n",
        "#store results of each episode in a list and use while plotting\n",
        "episode_durations = []\n",
        "\n",
        "mr_x_win = 0\n",
        "detectives_win = 0\n",
        "exploration_rate = []\n",
        "performance = []\n",
        "game_number = []\n",
        "for episode in range(num_episodes):\n",
        "    #reset the environmet\n",
        "    em = Environment(start_state[0], start_state[1], start_state[2], start_state[3])\n",
        "    \n",
        "    #getting initial state\n",
        "    state = em.get_state(0)\n",
        "\n",
        "    for timestep in range(25):\n",
        "      #print ('Timestep-->', timestep, ' x position', em.x_position)\n",
        "      #print ('Detective Position', em.detective_positions)\n",
        "      # print ('Mr. X options',em.board[em.x_position])\n",
        "      \n",
        "      action_tensor, rate = agent.select_action(state, policy_net, em.board[em.x_position])\n",
        "\n",
        "      action = action_tensor.tolist()\n",
        "      #print('Move to take for Mr. X', agent.mappings[action[0]])\n",
        "      \n",
        "      reward = em.take_action(action_tensor, timestep)\n",
        "      #print ('Reward',reward)\n",
        "      \n",
        "      next_state = em.get_state(timestep)\n",
        "      #print ('Next State in the form of feature vector',next_state)\n",
        "      \n",
        "      #memory.push(Experience(torch.tensor(state), action_tensor, torch.tensor(next_state), torch.tensor([reward])))\n",
        "      memory.push(Experience(state, action_tensor, next_state, torch.tensor(reward)))\n",
        "      #print ('MEMORY',memory)\n",
        "      state = next_state\n",
        "\n",
        "      if memory.can_provide_sample(batch_size):\n",
        "        #print ('in training')\n",
        "        experiences = memory.sample(batch_size)\n",
        "        states, actions, rewards, next_states = extract_tensors(experiences)\n",
        "        \n",
        "        current_q_values = QValues.get_current(policy_net, states, actions)\n",
        "        #print ('Current Q values',current_q_values)\n",
        "\n",
        "        next_q_values = QValues.get_next(target_net, next_states)\n",
        "        #print ('Next Q values', next_q_values)\n",
        "\n",
        "        target_q_values = (next_q_values * gamma) + rewards\n",
        "        #print ('Target', target_q_values)\n",
        "\n",
        "        loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      \n",
        "   \n",
        "      if em.is_done(timestep) == 2 :\n",
        "        mr_x_win += 1\n",
        "        exploration_rate.append(rate)\n",
        "        performance.append(detectives_win - mr_x_win)\n",
        "        game_number.append(episode)\n",
        "        break\n",
        "      elif(em.is_done(timestep) == 1):\n",
        "        detectives_win += 1\n",
        "        exploration_rate.append(rate)\n",
        "        performance.append(detectives_win - mr_x_win)\n",
        "        game_number.append(episode)\n",
        "        #plot(episode_durations, 100)\n",
        "        break\n",
        "      \n",
        "      if episode % target_update == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "    \n",
        "    print(\"Episode number: \", episode)\n",
        "print(\"Total number of episodes: \", num_episodes)\n",
        "print(\"Mr. X wins: \", mr_x_win)\n",
        "print(\"Detectives wins: \", detectives_win)\n",
        "\n",
        "print (performance)\n",
        "print (exploration_rate)\n",
        "plot_training(exploration_rate, performance)\n",
        "\n",
        "#save the model"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode number:  0\n",
            "Episode number:  1\n",
            "Episode number:  2\n",
            "Episode number:  3\n",
            "Episode number:  4\n",
            "Episode number:  5\n",
            "Episode number:  6\n",
            "Episode number:  7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-137b63c18744>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_q_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcm9q8dwiXqc",
        "colab_type": "code",
        "outputId": "5020dbd3-7804-42bb-dcb9-aae40e697a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# SAVING THE MODEL\n",
        "!ls /content\n",
        "torch.save(policy_net, '/content/mr_x_model.pth')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mr_x_model.pth\tsample_data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DQN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7--sEDIrAzkn",
        "colab_type": "code",
        "outputId": "3f2727bd-0236-44cd-be47-349c11e66f54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# LOADING THE MODEL - 1000 episodes\n",
        "model = torch.load('mr_x_model.pth')\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DQN(\n",
              "  (ll1): Linear(in_features=1211, out_features=708, bias=True)\n",
              "  (ll2): Linear(in_features=708, out_features=708, bias=True)\n",
              "  (ll3): Linear(in_features=708, out_features=354, bias=True)\n",
              "  (ll4): Linear(in_features=354, out_features=354, bias=True)\n",
              "  (oll): Linear(in_features=354, out_features=200, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAOxdsfcgk9A",
        "colab_type": "code",
        "outputId": "e253a32b-c67f-4ffc-e381-9b291b0333aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TESTING CODE\n",
        "\n",
        "batch_size = 256 #256\n",
        "gamma = 0.999\n",
        "eps_start = 1\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.001\n",
        "target_update = 10 # how often we update target with policy n/w\n",
        "memory_size = 100000 #check with paper\n",
        "lr = 0.001\n",
        "num_episodes = 1000 #1000\n",
        "# state_dim = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "start_state = [154, 5, [34, 29, 117, 174, 112], [[10, 8, 4], [10, 8, 4], [10, 8, 4], [10, 8, 4], [10, 8, 4]], 1]\n",
        "start_feature = generate_feature_space(start_state)\n",
        "\n",
        "em = Environment(start_state[0], start_state[1], start_state[2], start_state[3])\n",
        "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
        "\n",
        "agent = Agent(strategy, em.num_actions_available(), device)\n",
        "#print ('No. of available actions: ', em.num_actions_available())\n",
        "memory = ReplayMemory(memory_size)\n",
        "\n",
        "policy_net = model\n",
        "\n",
        "mr_x_win = 0\n",
        "detectives_win = 0\n",
        "performance = []\n",
        "game_number = []\n",
        "for episode in range(num_episodes):\n",
        "    #reset the environmet\n",
        "    em = Environment(start_state[0], start_state[1], start_state[2], start_state[3])\n",
        "    #print ('Reset',em.x_position)\n",
        "    \n",
        "    #getting initial state\n",
        "    state = em.get_state(0)\n",
        "\n",
        "    for timestep in range(25):\n",
        "      action_tensor = agent.select_action_testing(state, policy_net, em.board[em.x_position])\n",
        "\n",
        "      action = action_tensor.tolist()\n",
        "      #print('Move to take for Mr. X', agent.mappings[action[0]])\n",
        "      \n",
        "      reward = em.take_action(action_tensor, timestep)\n",
        "      #print ('Reward',reward)\n",
        "      \n",
        "      next_state = em.get_state(timestep)\n",
        "      #print ('Next State in the form of feature vector',next_state)\n",
        "      \n",
        "      state = next_state\n",
        "   \n",
        "      if em.is_done(timestep) == 2 :\n",
        "        mr_x_win += 1\n",
        "        performance.append(detectives_win - mr_x_win)\n",
        "        game_number.append(episode)\n",
        "        break\n",
        "\n",
        "      elif(em.is_done(timestep) == 1):\n",
        "        detectives_win += 1\n",
        "        performance.append(detectives_win - mr_x_win)\n",
        "        game_number.append(episode)\n",
        "        #plot(episode_durations, 100)\n",
        "        break\n",
        "    \n",
        "    print(\"Episode number: \", episode)\n",
        "\n",
        "print(\"Total number of episodes: \", num_episodes)\n",
        "print(\"Mr. X wins: \", mr_x_win)\n",
        "print(\"Detectives wins: \", detectives_win)\n",
        "\n",
        "print (performance)\n",
        "print (game_number)\n",
        "plot_testing(performance, game_number)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode number:  0\n",
            "Episode number:  1\n",
            "Episode number:  2\n",
            "Episode number:  3\n",
            "Episode number:  4\n",
            "Episode number:  5\n",
            "Episode number:  6\n",
            "Episode number:  7\n",
            "Episode number:  8\n",
            "Episode number:  9\n",
            "Episode number:  10\n",
            "Episode number:  11\n",
            "Episode number:  12\n",
            "Episode number:  13\n",
            "Episode number:  14\n",
            "Episode number:  15\n",
            "Episode number:  16\n",
            "Episode number:  17\n",
            "Episode number:  18\n",
            "Episode number:  19\n",
            "Episode number:  20\n",
            "Episode number:  21\n",
            "Episode number:  22\n",
            "Episode number:  23\n",
            "Episode number:  24\n",
            "Episode number:  25\n",
            "Episode number:  26\n",
            "Episode number:  27\n",
            "Episode number:  28\n",
            "Episode number:  29\n",
            "Episode number:  30\n",
            "Episode number:  31\n",
            "Episode number:  32\n",
            "Episode number:  33\n",
            "Episode number:  34\n",
            "Episode number:  35\n",
            "Episode number:  36\n",
            "Episode number:  37\n",
            "Episode number:  38\n",
            "Episode number:  39\n",
            "Episode number:  40\n",
            "Episode number:  41\n",
            "Episode number:  42\n",
            "Episode number:  43\n",
            "Episode number:  44\n",
            "Episode number:  45\n",
            "Episode number:  46\n",
            "Episode number:  47\n",
            "Episode number:  48\n",
            "Episode number:  49\n",
            "Episode number:  50\n",
            "Episode number:  51\n",
            "Episode number:  52\n",
            "Episode number:  53\n",
            "Episode number:  54\n",
            "Episode number:  55\n",
            "Episode number:  56\n",
            "Episode number:  57\n",
            "Episode number:  58\n",
            "Episode number:  59\n",
            "Episode number:  60\n",
            "Episode number:  61\n",
            "Episode number:  62\n",
            "Episode number:  63\n",
            "Episode number:  64\n",
            "Episode number:  65\n",
            "Episode number:  66\n",
            "Episode number:  67\n",
            "Episode number:  68\n",
            "Episode number:  69\n",
            "Episode number:  70\n",
            "Episode number:  71\n",
            "Episode number:  72\n",
            "Episode number:  73\n",
            "Episode number:  74\n",
            "Episode number:  75\n",
            "Episode number:  76\n",
            "Episode number:  77\n",
            "Episode number:  78\n",
            "Episode number:  79\n",
            "Episode number:  80\n",
            "Episode number:  81\n",
            "Episode number:  82\n",
            "Episode number:  83\n",
            "Episode number:  84\n",
            "Episode number:  85\n",
            "Episode number:  86\n",
            "Episode number:  87\n",
            "Episode number:  88\n",
            "Episode number:  89\n",
            "Episode number:  90\n",
            "Episode number:  91\n",
            "Episode number:  92\n",
            "Episode number:  93\n",
            "Episode number:  94\n",
            "Episode number:  95\n",
            "Episode number:  96\n",
            "Episode number:  97\n",
            "Episode number:  98\n",
            "Episode number:  99\n",
            "Episode number:  100\n",
            "Episode number:  101\n",
            "Episode number:  102\n",
            "Episode number:  103\n",
            "Episode number:  104\n",
            "Episode number:  105\n",
            "Episode number:  106\n",
            "Episode number:  107\n",
            "Episode number:  108\n",
            "Episode number:  109\n",
            "Episode number:  110\n",
            "Episode number:  111\n",
            "Episode number:  112\n",
            "Episode number:  113\n",
            "Episode number:  114\n",
            "Episode number:  115\n",
            "Episode number:  116\n",
            "Episode number:  117\n",
            "Episode number:  118\n",
            "Episode number:  119\n",
            "Episode number:  120\n",
            "Episode number:  121\n",
            "Episode number:  122\n",
            "Episode number:  123\n",
            "Episode number:  124\n",
            "Episode number:  125\n",
            "Episode number:  126\n",
            "Episode number:  127\n",
            "Episode number:  128\n",
            "Episode number:  129\n",
            "Episode number:  130\n",
            "Episode number:  131\n",
            "Episode number:  132\n",
            "Episode number:  133\n",
            "Episode number:  134\n",
            "Episode number:  135\n",
            "Episode number:  136\n",
            "Episode number:  137\n",
            "Episode number:  138\n",
            "Episode number:  139\n",
            "Episode number:  140\n",
            "Episode number:  141\n",
            "Episode number:  142\n",
            "Episode number:  143\n",
            "Episode number:  144\n",
            "Episode number:  145\n",
            "Episode number:  146\n",
            "Episode number:  147\n",
            "Episode number:  148\n",
            "Episode number:  149\n",
            "Episode number:  150\n",
            "Episode number:  151\n",
            "Episode number:  152\n",
            "Episode number:  153\n",
            "Episode number:  154\n",
            "Episode number:  155\n",
            "Episode number:  156\n",
            "Episode number:  157\n",
            "Episode number:  158\n",
            "Episode number:  159\n",
            "Episode number:  160\n",
            "Episode number:  161\n",
            "Episode number:  162\n",
            "Episode number:  163\n",
            "Episode number:  164\n",
            "Episode number:  165\n",
            "Episode number:  166\n",
            "Episode number:  167\n",
            "Episode number:  168\n",
            "Episode number:  169\n",
            "Episode number:  170\n",
            "Episode number:  171\n",
            "Episode number:  172\n",
            "Episode number:  173\n",
            "Episode number:  174\n",
            "Episode number:  175\n",
            "Episode number:  176\n",
            "Episode number:  177\n",
            "Episode number:  178\n",
            "Episode number:  179\n",
            "Episode number:  180\n",
            "Episode number:  181\n",
            "Episode number:  182\n",
            "Episode number:  183\n",
            "Episode number:  184\n",
            "Episode number:  185\n",
            "Episode number:  186\n",
            "Episode number:  187\n",
            "Episode number:  188\n",
            "Episode number:  189\n",
            "Episode number:  190\n",
            "Episode number:  191\n",
            "Episode number:  192\n",
            "Episode number:  193\n",
            "Episode number:  194\n",
            "Episode number:  195\n",
            "Episode number:  196\n",
            "Episode number:  197\n",
            "Episode number:  198\n",
            "Episode number:  199\n",
            "Episode number:  200\n",
            "Episode number:  201\n",
            "Episode number:  202\n",
            "Episode number:  203\n",
            "Episode number:  204\n",
            "Episode number:  205\n",
            "Episode number:  206\n",
            "Episode number:  207\n",
            "Episode number:  208\n",
            "Episode number:  209\n",
            "Episode number:  210\n",
            "Episode number:  211\n",
            "Episode number:  212\n",
            "Episode number:  213\n",
            "Episode number:  214\n",
            "Episode number:  215\n",
            "Episode number:  216\n",
            "Episode number:  217\n",
            "Episode number:  218\n",
            "Episode number:  219\n",
            "Episode number:  220\n",
            "Episode number:  221\n",
            "Episode number:  222\n",
            "Episode number:  223\n",
            "Episode number:  224\n",
            "Episode number:  225\n",
            "Episode number:  226\n",
            "Episode number:  227\n",
            "Episode number:  228\n",
            "Episode number:  229\n",
            "Episode number:  230\n",
            "Episode number:  231\n",
            "Episode number:  232\n",
            "Episode number:  233\n",
            "Episode number:  234\n",
            "Episode number:  235\n",
            "Episode number:  236\n",
            "Episode number:  237\n",
            "Episode number:  238\n",
            "Episode number:  239\n",
            "Episode number:  240\n",
            "Episode number:  241\n",
            "Episode number:  242\n",
            "Episode number:  243\n",
            "Episode number:  244\n",
            "Episode number:  245\n",
            "Episode number:  246\n",
            "Episode number:  247\n",
            "Episode number:  248\n",
            "Episode number:  249\n",
            "Episode number:  250\n",
            "Episode number:  251\n",
            "Episode number:  252\n",
            "Episode number:  253\n",
            "Episode number:  254\n",
            "Episode number:  255\n",
            "Episode number:  256\n",
            "Episode number:  257\n",
            "Episode number:  258\n",
            "Episode number:  259\n",
            "Episode number:  260\n",
            "Episode number:  261\n",
            "Episode number:  262\n",
            "Episode number:  263\n",
            "Episode number:  264\n",
            "Episode number:  265\n",
            "Episode number:  266\n",
            "Episode number:  267\n",
            "Episode number:  268\n",
            "Episode number:  269\n",
            "Episode number:  270\n",
            "Episode number:  271\n",
            "Episode number:  272\n",
            "Episode number:  273\n",
            "Episode number:  274\n",
            "Episode number:  275\n",
            "Episode number:  276\n",
            "Episode number:  277\n",
            "Episode number:  278\n",
            "Episode number:  279\n",
            "Episode number:  280\n",
            "Episode number:  281\n",
            "Episode number:  282\n",
            "Episode number:  283\n",
            "Episode number:  284\n",
            "Episode number:  285\n",
            "Episode number:  286\n",
            "Episode number:  287\n",
            "Episode number:  288\n",
            "Episode number:  289\n",
            "Episode number:  290\n",
            "Episode number:  291\n",
            "Episode number:  292\n",
            "Episode number:  293\n",
            "Episode number:  294\n",
            "Episode number:  295\n",
            "Episode number:  296\n",
            "Episode number:  297\n",
            "Episode number:  298\n",
            "Episode number:  299\n",
            "Episode number:  300\n",
            "Episode number:  301\n",
            "Episode number:  302\n",
            "Episode number:  303\n",
            "Episode number:  304\n",
            "Episode number:  305\n",
            "Episode number:  306\n",
            "Episode number:  307\n",
            "Episode number:  308\n",
            "Episode number:  309\n",
            "Episode number:  310\n",
            "Episode number:  311\n",
            "Episode number:  312\n",
            "Episode number:  313\n",
            "Episode number:  314\n",
            "Episode number:  315\n",
            "Episode number:  316\n",
            "Episode number:  317\n",
            "Episode number:  318\n",
            "Episode number:  319\n",
            "Episode number:  320\n",
            "Episode number:  321\n",
            "Episode number:  322\n",
            "Episode number:  323\n",
            "Episode number:  324\n",
            "Episode number:  325\n",
            "Episode number:  326\n",
            "Episode number:  327\n",
            "Episode number:  328\n",
            "Episode number:  329\n",
            "Episode number:  330\n",
            "Episode number:  331\n",
            "Episode number:  332\n",
            "Episode number:  333\n",
            "Episode number:  334\n",
            "Episode number:  335\n",
            "Episode number:  336\n",
            "Episode number:  337\n",
            "Episode number:  338\n",
            "Episode number:  339\n",
            "Episode number:  340\n",
            "Episode number:  341\n",
            "Episode number:  342\n",
            "Episode number:  343\n",
            "Episode number:  344\n",
            "Episode number:  345\n",
            "Episode number:  346\n",
            "Episode number:  347\n",
            "Episode number:  348\n",
            "Episode number:  349\n",
            "Episode number:  350\n",
            "Episode number:  351\n",
            "Episode number:  352\n",
            "Episode number:  353\n",
            "Episode number:  354\n",
            "Episode number:  355\n",
            "Episode number:  356\n",
            "Episode number:  357\n",
            "Episode number:  358\n",
            "Episode number:  359\n",
            "Episode number:  360\n",
            "Episode number:  361\n",
            "Episode number:  362\n",
            "Episode number:  363\n",
            "Episode number:  364\n",
            "Episode number:  365\n",
            "Episode number:  366\n",
            "Episode number:  367\n",
            "Episode number:  368\n",
            "Episode number:  369\n",
            "Episode number:  370\n",
            "Episode number:  371\n",
            "Episode number:  372\n",
            "Episode number:  373\n",
            "Episode number:  374\n",
            "Episode number:  375\n",
            "Episode number:  376\n",
            "Episode number:  377\n",
            "Episode number:  378\n",
            "Episode number:  379\n",
            "Episode number:  380\n",
            "Episode number:  381\n",
            "Episode number:  382\n",
            "Episode number:  383\n",
            "Episode number:  384\n",
            "Episode number:  385\n",
            "Episode number:  386\n",
            "Episode number:  387\n",
            "Episode number:  388\n",
            "Episode number:  389\n",
            "Episode number:  390\n",
            "Episode number:  391\n",
            "Episode number:  392\n",
            "Episode number:  393\n",
            "Episode number:  394\n",
            "Episode number:  395\n",
            "Episode number:  396\n",
            "Episode number:  397\n",
            "Episode number:  398\n",
            "Episode number:  399\n",
            "Episode number:  400\n",
            "Episode number:  401\n",
            "Episode number:  402\n",
            "Episode number:  403\n",
            "Episode number:  404\n",
            "Episode number:  405\n",
            "Episode number:  406\n",
            "Episode number:  407\n",
            "Episode number:  408\n",
            "Episode number:  409\n",
            "Episode number:  410\n",
            "Episode number:  411\n",
            "Episode number:  412\n",
            "Episode number:  413\n",
            "Episode number:  414\n",
            "Episode number:  415\n",
            "Episode number:  416\n",
            "Episode number:  417\n",
            "Episode number:  418\n",
            "Episode number:  419\n",
            "Episode number:  420\n",
            "Episode number:  421\n",
            "Episode number:  422\n",
            "Episode number:  423\n",
            "Episode number:  424\n",
            "Episode number:  425\n",
            "Episode number:  426\n",
            "Episode number:  427\n",
            "Episode number:  428\n",
            "Episode number:  429\n",
            "Episode number:  430\n",
            "Episode number:  431\n",
            "Episode number:  432\n",
            "Episode number:  433\n",
            "Episode number:  434\n",
            "Episode number:  435\n",
            "Episode number:  436\n",
            "Episode number:  437\n",
            "Episode number:  438\n",
            "Episode number:  439\n",
            "Episode number:  440\n",
            "Episode number:  441\n",
            "Episode number:  442\n",
            "Episode number:  443\n",
            "Episode number:  444\n",
            "Episode number:  445\n",
            "Episode number:  446\n",
            "Episode number:  447\n",
            "Episode number:  448\n",
            "Episode number:  449\n",
            "Episode number:  450\n",
            "Episode number:  451\n",
            "Episode number:  452\n",
            "Episode number:  453\n",
            "Episode number:  454\n",
            "Episode number:  455\n",
            "Episode number:  456\n",
            "Episode number:  457\n",
            "Episode number:  458\n",
            "Episode number:  459\n",
            "Episode number:  460\n",
            "Episode number:  461\n",
            "Episode number:  462\n",
            "Episode number:  463\n",
            "Episode number:  464\n",
            "Episode number:  465\n",
            "Episode number:  466\n",
            "Episode number:  467\n",
            "Episode number:  468\n",
            "Episode number:  469\n",
            "Episode number:  470\n",
            "Episode number:  471\n",
            "Episode number:  472\n",
            "Episode number:  473\n",
            "Episode number:  474\n",
            "Episode number:  475\n",
            "Episode number:  476\n",
            "Episode number:  477\n",
            "Episode number:  478\n",
            "Episode number:  479\n",
            "Episode number:  480\n",
            "Episode number:  481\n",
            "Episode number:  482\n",
            "Episode number:  483\n",
            "Episode number:  484\n",
            "Episode number:  485\n",
            "Episode number:  486\n",
            "Episode number:  487\n",
            "Episode number:  488\n",
            "Episode number:  489\n",
            "Episode number:  490\n",
            "Episode number:  491\n",
            "Episode number:  492\n",
            "Episode number:  493\n",
            "Episode number:  494\n",
            "Episode number:  495\n",
            "Episode number:  496\n",
            "Episode number:  497\n",
            "Episode number:  498\n",
            "Episode number:  499\n",
            "Episode number:  500\n",
            "Episode number:  501\n",
            "Episode number:  502\n",
            "Episode number:  503\n",
            "Episode number:  504\n",
            "Episode number:  505\n",
            "Episode number:  506\n",
            "Episode number:  507\n",
            "Episode number:  508\n",
            "Episode number:  509\n",
            "Episode number:  510\n",
            "Episode number:  511\n",
            "Episode number:  512\n",
            "Episode number:  513\n",
            "Episode number:  514\n",
            "Episode number:  515\n",
            "Episode number:  516\n",
            "Episode number:  517\n",
            "Episode number:  518\n",
            "Episode number:  519\n",
            "Episode number:  520\n",
            "Episode number:  521\n",
            "Episode number:  522\n",
            "Episode number:  523\n",
            "Episode number:  524\n",
            "Episode number:  525\n",
            "Episode number:  526\n",
            "Episode number:  527\n",
            "Episode number:  528\n",
            "Episode number:  529\n",
            "Episode number:  530\n",
            "Episode number:  531\n",
            "Episode number:  532\n",
            "Episode number:  533\n",
            "Episode number:  534\n",
            "Episode number:  535\n",
            "Episode number:  536\n",
            "Episode number:  537\n",
            "Episode number:  538\n",
            "Episode number:  539\n",
            "Episode number:  540\n",
            "Episode number:  541\n",
            "Episode number:  542\n",
            "Episode number:  543\n",
            "Episode number:  544\n",
            "Episode number:  545\n",
            "Episode number:  546\n",
            "Episode number:  547\n",
            "Episode number:  548\n",
            "Episode number:  549\n",
            "Episode number:  550\n",
            "Episode number:  551\n",
            "Episode number:  552\n",
            "Episode number:  553\n",
            "Episode number:  554\n",
            "Episode number:  555\n",
            "Episode number:  556\n",
            "Episode number:  557\n",
            "Episode number:  558\n",
            "Episode number:  559\n",
            "Episode number:  560\n",
            "Episode number:  561\n",
            "Episode number:  562\n",
            "Episode number:  563\n",
            "Episode number:  564\n",
            "Episode number:  565\n",
            "Episode number:  566\n",
            "Episode number:  567\n",
            "Episode number:  568\n",
            "Episode number:  569\n",
            "Episode number:  570\n",
            "Episode number:  571\n",
            "Episode number:  572\n",
            "Episode number:  573\n",
            "Episode number:  574\n",
            "Episode number:  575\n",
            "Episode number:  576\n",
            "Episode number:  577\n",
            "Episode number:  578\n",
            "Episode number:  579\n",
            "Episode number:  580\n",
            "Episode number:  581\n",
            "Episode number:  582\n",
            "Episode number:  583\n",
            "Episode number:  584\n",
            "Episode number:  585\n",
            "Episode number:  586\n",
            "Episode number:  587\n",
            "Episode number:  588\n",
            "Episode number:  589\n",
            "Episode number:  590\n",
            "Episode number:  591\n",
            "Episode number:  592\n",
            "Episode number:  593\n",
            "Episode number:  594\n",
            "Episode number:  595\n",
            "Episode number:  596\n",
            "Episode number:  597\n",
            "Episode number:  598\n",
            "Episode number:  599\n",
            "Episode number:  600\n",
            "Episode number:  601\n",
            "Episode number:  602\n",
            "Episode number:  603\n",
            "Episode number:  604\n",
            "Episode number:  605\n",
            "Episode number:  606\n",
            "Episode number:  607\n",
            "Episode number:  608\n",
            "Episode number:  609\n",
            "Episode number:  610\n",
            "Episode number:  611\n",
            "Episode number:  612\n",
            "Episode number:  613\n",
            "Episode number:  614\n",
            "Episode number:  615\n",
            "Episode number:  616\n",
            "Episode number:  617\n",
            "Episode number:  618\n",
            "Episode number:  619\n",
            "Episode number:  620\n",
            "Episode number:  621\n",
            "Episode number:  622\n",
            "Episode number:  623\n",
            "Episode number:  624\n",
            "Episode number:  625\n",
            "Episode number:  626\n",
            "Episode number:  627\n",
            "Episode number:  628\n",
            "Episode number:  629\n",
            "Episode number:  630\n",
            "Episode number:  631\n",
            "Episode number:  632\n",
            "Episode number:  633\n",
            "Episode number:  634\n",
            "Episode number:  635\n",
            "Episode number:  636\n",
            "Episode number:  637\n",
            "Episode number:  638\n",
            "Episode number:  639\n",
            "Episode number:  640\n",
            "Episode number:  641\n",
            "Episode number:  642\n",
            "Episode number:  643\n",
            "Episode number:  644\n",
            "Episode number:  645\n",
            "Episode number:  646\n",
            "Episode number:  647\n",
            "Episode number:  648\n",
            "Episode number:  649\n",
            "Episode number:  650\n",
            "Episode number:  651\n",
            "Episode number:  652\n",
            "Episode number:  653\n",
            "Episode number:  654\n",
            "Episode number:  655\n",
            "Episode number:  656\n",
            "Episode number:  657\n",
            "Episode number:  658\n",
            "Episode number:  659\n",
            "Episode number:  660\n",
            "Episode number:  661\n",
            "Episode number:  662\n",
            "Episode number:  663\n",
            "Episode number:  664\n",
            "Episode number:  665\n",
            "Episode number:  666\n",
            "Episode number:  667\n",
            "Episode number:  668\n",
            "Episode number:  669\n",
            "Episode number:  670\n",
            "Episode number:  671\n",
            "Episode number:  672\n",
            "Episode number:  673\n",
            "Episode number:  674\n",
            "Episode number:  675\n",
            "Episode number:  676\n",
            "Episode number:  677\n",
            "Episode number:  678\n",
            "Episode number:  679\n",
            "Episode number:  680\n",
            "Episode number:  681\n",
            "Episode number:  682\n",
            "Episode number:  683\n",
            "Episode number:  684\n",
            "Episode number:  685\n",
            "Episode number:  686\n",
            "Episode number:  687\n",
            "Episode number:  688\n",
            "Episode number:  689\n",
            "Episode number:  690\n",
            "Episode number:  691\n",
            "Episode number:  692\n",
            "Episode number:  693\n",
            "Episode number:  694\n",
            "Episode number:  695\n",
            "Episode number:  696\n",
            "Episode number:  697\n",
            "Episode number:  698\n",
            "Episode number:  699\n",
            "Episode number:  700\n",
            "Episode number:  701\n",
            "Episode number:  702\n",
            "Episode number:  703\n",
            "Episode number:  704\n",
            "Episode number:  705\n",
            "Episode number:  706\n",
            "Episode number:  707\n",
            "Episode number:  708\n",
            "Episode number:  709\n",
            "Episode number:  710\n",
            "Episode number:  711\n",
            "Episode number:  712\n",
            "Episode number:  713\n",
            "Episode number:  714\n",
            "Episode number:  715\n",
            "Episode number:  716\n",
            "Episode number:  717\n",
            "Episode number:  718\n",
            "Episode number:  719\n",
            "Episode number:  720\n",
            "Episode number:  721\n",
            "Episode number:  722\n",
            "Episode number:  723\n",
            "Episode number:  724\n",
            "Episode number:  725\n",
            "Episode number:  726\n",
            "Episode number:  727\n",
            "Episode number:  728\n",
            "Episode number:  729\n",
            "Episode number:  730\n",
            "Episode number:  731\n",
            "Episode number:  732\n",
            "Episode number:  733\n",
            "Episode number:  734\n",
            "Episode number:  735\n",
            "Episode number:  736\n",
            "Episode number:  737\n",
            "Episode number:  738\n",
            "Episode number:  739\n",
            "Episode number:  740\n",
            "Episode number:  741\n",
            "Episode number:  742\n",
            "Episode number:  743\n",
            "Episode number:  744\n",
            "Episode number:  745\n",
            "Episode number:  746\n",
            "Episode number:  747\n",
            "Episode number:  748\n",
            "Episode number:  749\n",
            "Episode number:  750\n",
            "Episode number:  751\n",
            "Episode number:  752\n",
            "Episode number:  753\n",
            "Episode number:  754\n",
            "Episode number:  755\n",
            "Episode number:  756\n",
            "Episode number:  757\n",
            "Episode number:  758\n",
            "Episode number:  759\n",
            "Episode number:  760\n",
            "Episode number:  761\n",
            "Episode number:  762\n",
            "Episode number:  763\n",
            "Episode number:  764\n",
            "Episode number:  765\n",
            "Episode number:  766\n",
            "Episode number:  767\n",
            "Episode number:  768\n",
            "Episode number:  769\n",
            "Episode number:  770\n",
            "Episode number:  771\n",
            "Episode number:  772\n",
            "Episode number:  773\n",
            "Episode number:  774\n",
            "Episode number:  775\n",
            "Episode number:  776\n",
            "Episode number:  777\n",
            "Episode number:  778\n",
            "Episode number:  779\n",
            "Episode number:  780\n",
            "Episode number:  781\n",
            "Episode number:  782\n",
            "Episode number:  783\n",
            "Episode number:  784\n",
            "Episode number:  785\n",
            "Episode number:  786\n",
            "Episode number:  787\n",
            "Episode number:  788\n",
            "Episode number:  789\n",
            "Episode number:  790\n",
            "Episode number:  791\n",
            "Episode number:  792\n",
            "Episode number:  793\n",
            "Episode number:  794\n",
            "Episode number:  795\n",
            "Episode number:  796\n",
            "Episode number:  797\n",
            "Episode number:  798\n",
            "Episode number:  799\n",
            "Episode number:  800\n",
            "Episode number:  801\n",
            "Episode number:  802\n",
            "Episode number:  803\n",
            "Episode number:  804\n",
            "Episode number:  805\n",
            "Episode number:  806\n",
            "Episode number:  807\n",
            "Episode number:  808\n",
            "Episode number:  809\n",
            "Episode number:  810\n",
            "Episode number:  811\n",
            "Episode number:  812\n",
            "Episode number:  813\n",
            "Episode number:  814\n",
            "Episode number:  815\n",
            "Episode number:  816\n",
            "Episode number:  817\n",
            "Episode number:  818\n",
            "Episode number:  819\n",
            "Episode number:  820\n",
            "Episode number:  821\n",
            "Episode number:  822\n",
            "Episode number:  823\n",
            "Episode number:  824\n",
            "Episode number:  825\n",
            "Episode number:  826\n",
            "Episode number:  827\n",
            "Episode number:  828\n",
            "Episode number:  829\n",
            "Episode number:  830\n",
            "Episode number:  831\n",
            "Episode number:  832\n",
            "Episode number:  833\n",
            "Episode number:  834\n",
            "Episode number:  835\n",
            "Episode number:  836\n",
            "Episode number:  837\n",
            "Episode number:  838\n",
            "Episode number:  839\n",
            "Episode number:  840\n",
            "Episode number:  841\n",
            "Episode number:  842\n",
            "Episode number:  843\n",
            "Episode number:  844\n",
            "Episode number:  845\n",
            "Episode number:  846\n",
            "Episode number:  847\n",
            "Episode number:  848\n",
            "Episode number:  849\n",
            "Episode number:  850\n",
            "Episode number:  851\n",
            "Episode number:  852\n",
            "Episode number:  853\n",
            "Episode number:  854\n",
            "Episode number:  855\n",
            "Episode number:  856\n",
            "Episode number:  857\n",
            "Episode number:  858\n",
            "Episode number:  859\n",
            "Episode number:  860\n",
            "Episode number:  861\n",
            "Episode number:  862\n",
            "Episode number:  863\n",
            "Episode number:  864\n",
            "Episode number:  865\n",
            "Episode number:  866\n",
            "Episode number:  867\n",
            "Episode number:  868\n",
            "Episode number:  869\n",
            "Episode number:  870\n",
            "Episode number:  871\n",
            "Episode number:  872\n",
            "Episode number:  873\n",
            "Episode number:  874\n",
            "Episode number:  875\n",
            "Episode number:  876\n",
            "Episode number:  877\n",
            "Episode number:  878\n",
            "Episode number:  879\n",
            "Episode number:  880\n",
            "Episode number:  881\n",
            "Episode number:  882\n",
            "Episode number:  883\n",
            "Episode number:  884\n",
            "Episode number:  885\n",
            "Episode number:  886\n",
            "Episode number:  887\n",
            "Episode number:  888\n",
            "Episode number:  889\n",
            "Episode number:  890\n",
            "Episode number:  891\n",
            "Episode number:  892\n",
            "Episode number:  893\n",
            "Episode number:  894\n",
            "Episode number:  895\n",
            "Episode number:  896\n",
            "Episode number:  897\n",
            "Episode number:  898\n",
            "Episode number:  899\n",
            "Episode number:  900\n",
            "Episode number:  901\n",
            "Episode number:  902\n",
            "Episode number:  903\n",
            "Episode number:  904\n",
            "Episode number:  905\n",
            "Episode number:  906\n",
            "Episode number:  907\n",
            "Episode number:  908\n",
            "Episode number:  909\n",
            "Episode number:  910\n",
            "Episode number:  911\n",
            "Episode number:  912\n",
            "Episode number:  913\n",
            "Episode number:  914\n",
            "Episode number:  915\n",
            "Episode number:  916\n",
            "Episode number:  917\n",
            "Episode number:  918\n",
            "Episode number:  919\n",
            "Episode number:  920\n",
            "Episode number:  921\n",
            "Episode number:  922\n",
            "Episode number:  923\n",
            "Episode number:  924\n",
            "Episode number:  925\n",
            "Episode number:  926\n",
            "Episode number:  927\n",
            "Episode number:  928\n",
            "Episode number:  929\n",
            "Episode number:  930\n",
            "Episode number:  931\n",
            "Episode number:  932\n",
            "Episode number:  933\n",
            "Episode number:  934\n",
            "Episode number:  935\n",
            "Episode number:  936\n",
            "Episode number:  937\n",
            "Episode number:  938\n",
            "Episode number:  939\n",
            "Episode number:  940\n",
            "Episode number:  941\n",
            "Episode number:  942\n",
            "Episode number:  943\n",
            "Episode number:  944\n",
            "Episode number:  945\n",
            "Episode number:  946\n",
            "Episode number:  947\n",
            "Episode number:  948\n",
            "Episode number:  949\n",
            "Episode number:  950\n",
            "Episode number:  951\n",
            "Episode number:  952\n",
            "Episode number:  953\n",
            "Episode number:  954\n",
            "Episode number:  955\n",
            "Episode number:  956\n",
            "Episode number:  957\n",
            "Episode number:  958\n",
            "Episode number:  959\n",
            "Episode number:  960\n",
            "Episode number:  961\n",
            "Episode number:  962\n",
            "Episode number:  963\n",
            "Episode number:  964\n",
            "Episode number:  965\n",
            "Episode number:  966\n",
            "Episode number:  967\n",
            "Episode number:  968\n",
            "Episode number:  969\n",
            "Episode number:  970\n",
            "Episode number:  971\n",
            "Episode number:  972\n",
            "Episode number:  973\n",
            "Episode number:  974\n",
            "Episode number:  975\n",
            "Episode number:  976\n",
            "Episode number:  977\n",
            "Episode number:  978\n",
            "Episode number:  979\n",
            "Episode number:  980\n",
            "Episode number:  981\n",
            "Episode number:  982\n",
            "Episode number:  983\n",
            "Episode number:  984\n",
            "Episode number:  985\n",
            "Episode number:  986\n",
            "Episode number:  987\n",
            "Episode number:  988\n",
            "Episode number:  989\n",
            "Episode number:  990\n",
            "Episode number:  991\n",
            "Episode number:  992\n",
            "Episode number:  993\n",
            "Episode number:  994\n",
            "Episode number:  995\n",
            "Episode number:  996\n",
            "Episode number:  997\n",
            "Episode number:  998\n",
            "Episode number:  999\n",
            "Total number of episodes:  1000\n",
            "Mr. X wins:  416\n",
            "Detectives wins:  584\n",
            "[-1, -2, -1, -2, -1, 0, 1, 2, 1, 0, 1, 2, 3, 2, 1, 0, -1, -2, -1, 0, -1, -2, -3, -4, -3, -2, -1, 0, 1, 0, 1, 0, -1, 0, 1, 2, 1, 0, -1, -2, -1, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 7, 8, 9, 10, 11, 12, 13, 14, 15, 14, 15, 14, 13, 12, 11, 10, 9, 8, 9, 8, 7, 8, 9, 8, 9, 10, 11, 12, 11, 12, 11, 12, 13, 12, 13, 12, 11, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 18, 19, 20, 21, 22, 21, 20, 21, 20, 19, 20, 19, 18, 19, 18, 19, 20, 21, 22, 23, 24, 25, 26, 25, 24, 23, 22, 23, 24, 25, 26, 27, 28, 29, 28, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 21, 20, 19, 18, 19, 20, 19, 18, 17, 18, 17, 16, 15, 14, 15, 16, 15, 16, 17, 18, 19, 20, 21, 22, 21, 20, 21, 20, 21, 22, 21, 22, 23, 24, 23, 24, 25, 26, 27, 26, 25, 26, 27, 26, 27, 26, 27, 28, 27, 28, 29, 30, 29, 28, 29, 30, 31, 32, 33, 32, 31, 32, 31, 32, 31, 30, 29, 28, 27, 26, 27, 28, 27, 28, 29, 30, 29, 30, 29, 30, 31, 32, 33, 34, 33, 32, 33, 34, 33, 32, 33, 34, 35, 34, 33, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 40, 39, 40, 39, 40, 41, 42, 41, 42, 43, 42, 43, 44, 45, 46, 45, 46, 47, 48, 49, 48, 47, 48, 49, 50, 51, 52, 53, 54, 53, 52, 51, 50, 51, 52, 51, 52, 51, 50, 49, 50, 51, 52, 53, 54, 53, 52, 53, 52, 53, 52, 51, 50, 49, 48, 47, 46, 45, 46, 45, 44, 45, 44, 43, 42, 43, 42, 43, 42, 43, 42, 43, 42, 41, 40, 41, 42, 41, 42, 43, 44, 45, 44, 45, 44, 43, 44, 45, 46, 47, 46, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 37, 38, 37, 36, 35, 34, 33, 34, 33, 34, 35, 34, 33, 32, 33, 34, 33, 34, 35, 36, 37, 38, 39, 40, 41, 40, 41, 42, 43, 44, 45, 46, 45, 46, 45, 44, 43, 42, 43, 42, 43, 44, 43, 42, 41, 42, 43, 44, 45, 44, 45, 46, 45, 46, 47, 46, 45, 44, 45, 44, 45, 46, 45, 44, 45, 46, 45, 46, 47, 48, 49, 50, 51, 50, 49, 50, 51, 50, 49, 48, 49, 48, 49, 50, 49, 50, 51, 52, 51, 52, 53, 54, 55, 56, 57, 58, 57, 58, 57, 58, 59, 60, 61, 62, 61, 60, 61, 62, 63, 64, 63, 64, 65, 66, 67, 68, 69, 68, 67, 66, 67, 66, 67, 68, 69, 70, 69, 68, 67, 66, 67, 66, 65, 66, 67, 68, 69, 68, 67, 68, 69, 68, 69, 70, 69, 70, 71, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 79, 78, 77, 76, 77, 76, 75, 74, 73, 74, 75, 76, 77, 78, 79, 78, 79, 78, 79, 78, 77, 78, 79, 80, 79, 80, 81, 82, 83, 82, 81, 82, 83, 84, 83, 82, 81, 82, 81, 82, 83, 84, 85, 86, 87, 88, 89, 88, 89, 90, 91, 92, 93, 94, 93, 94, 95, 96, 95, 94, 93, 92, 93, 92, 93, 92, 91, 92, 93, 94, 95, 96, 95, 96, 95, 94, 93, 92, 91, 92, 93, 94, 95, 96, 95, 96, 95, 96, 97, 96, 95, 96, 95, 96, 97, 98, 97, 98, 99, 98, 99, 100, 101, 100, 99, 100, 101, 102, 101, 102, 101, 100, 99, 100, 101, 100, 99, 100, 101, 102, 103, 104, 105, 104, 105, 104, 105, 104, 103, 102, 103, 104, 105, 106, 105, 104, 105, 104, 105, 106, 107, 108, 107, 108, 109, 110, 109, 108, 109, 110, 109, 110, 111, 112, 113, 114, 115, 116, 117, 116, 117, 116, 115, 116, 115, 114, 113, 114, 113, 112, 111, 110, 109, 108, 109, 108, 109, 108, 107, 106, 105, 104, 103, 104, 103, 104, 103, 102, 103, 104, 105, 104, 103, 104, 105, 104, 103, 104, 105, 106, 107, 106, 107, 106, 105, 106, 107, 108, 109, 110, 111, 110, 111, 112, 111, 110, 109, 108, 109, 108, 109, 110, 111, 112, 111, 112, 113, 114, 115, 114, 115, 114, 115, 116, 115, 116, 117, 118, 119, 120, 119, 118, 119, 120, 119, 120, 119, 118, 117, 116, 117, 118, 117, 118, 117, 116, 115, 116, 117, 116, 115, 114, 115, 114, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 121, 120, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 130, 131, 132, 133, 134, 133, 132, 131, 130, 129, 130, 131, 130, 131, 132, 133, 134, 135, 136, 137, 136, 135, 136, 137, 138, 139, 138, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 153, 152, 153, 152, 153, 152, 151, 152, 153, 152, 153, 154, 153, 152, 151, 150, 149, 148, 149, 148, 147, 148, 149, 150, 149, 150, 151, 152, 153, 154, 153, 154, 155, 156, 155, 154, 155, 154, 153, 152, 153, 154, 155, 154, 155, 154, 153, 154, 153, 152, 151, 152, 151, 152, 153, 154, 153, 152, 151, 152, 153, 154, 155, 154, 155, 156, 157, 158, 159, 158, 159, 160, 159, 158, 159, 160, 159, 158, 159, 158, 157, 158, 157, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 166, 165, 164, 163, 164, 163, 162, 161, 160, 159, 158, 157, 156, 157, 158, 157, 156, 157, 158, 157, 156, 157, 156, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 169, 168, 167, 166, 167, 166, 167, 166, 165, 164, 165, 166, 165, 166, 167, 168, 167, 168, 169, 168, 169, 168]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noqAgIiJVFBRERKSKgoKIiFT5f7Ce0MZfZUgFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFmnbgxjlMpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}